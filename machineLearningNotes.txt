
Machine Learning Notes:
----------------------
----------------------

Tutorial used here:
https://www.youtube.com/watch?v=gmvvaobm7eQ&list=PLeo1K3hjS3uvCeTYTeyfe0-rN5r8zn9rw


Tutorial 1 : Machine Learning Tutorial Python : What is Machine Learning?
--------------------------

We need to think from how the baby learn.
We are showing a cow to the baby and we say this is cow.
Then we are showing few more cows to the baby and we saw this is a cow
We will continue this.
Next time when baby see any cow, then can say it is cow.
Say to other things or products.

So here we are training the baby with different inputs of the same products multiple times for learning.
Then it learn and able to recognise.
Same will be doing in machine learning.
This process is deep learning. Deep learning is also a part of machine learning

We will using some mathametical concepts for achiving many things in machine learning.

Below are few examples of machines learning which are already there
1. Google gmail spam folder. Google automatically identify the spam folder and keep it in spam folder instead of inbox. Because it knows about spam folder which was alredy there is their system for long
2. Youtube recommendation videos
3. Even driverless cars or self driving cars


--------------------



Tutorial 2: Machine Learning Tutorial Python - 2: Linear Regression Single Variable
------------------------------------------------------------------------------------
This is the tutorial where we can learn how to predict the values when we have list of two pair values.
Here we have used the example of area in square feets and its price in a city
When we have few in the list, we can able to predict the value for any square fleet.

When one value is depend on one variable, below is the formula
y = mx + b
Here we give many sample input with list of two values. Based on this formula, it will predict for the given one


x is the coefficient(independent variable or feature)
b is the intercept
y is the is the dependent variable (which is target value)

Check the examples for more details

-------------------------


Tutorial 3: Machine Learning Tutorial Python - 3: Linear Regression Multiple Variables
---------------------------
Here for finding one value, it might be depend on multiple values.
Eg: calculating home price is depend on squarefeet, number of bedrooms, age of home etc
So when one taget value depend on multiple variable, we can go with below formula

y = m1x1 + m2x2 + m3x3 + b

Below is the formula for above example

price = m1 * area + m2 * bedrooms + m3 * age + b

Here area, bedrooms and age are independent variables
m1, m2 and m3 are coefficient
b is intercept
price is dependent variables

Independent variables are also called as features

We can have n number of independent variables and coefficient combinations

Below is to find the mean value 
df.ColumnName.mean()
or
df['columnName'].mean()    

Eg:
df['test_score(out of 10)'].mean()


math.floor(giveValue)    # this will give the roundedv value

Eg:
import math
mean_value = math.floor(df['test_score(out of 10)'].mean())

Below is to replace the null value with specific vaue (Mostly with 0 or mean value)
df.columnName = df.columnName.fillna(mean_value)
or
df['columnName'] = df['columnName'].fillna(mean_value)

Eg"
df['score'] = df['score'].fillname(0)    # to fill the empty value with 0
or
df['score'] = df['score'].fillname(mean_value)    # to fill the empty value with mean value


reg = linear_model.LinearRegression()
ref.fit(df[[area, bedroom, age]], price)    //Here we are adding the input to train,

ref.coef_        #To give coefficient
ref.intercept_   # To give intercept value

ref.predict([[4000, 3, 5]])   - This will print the predicted value as per input given. We should give same number of parameters and its values

----------------------------------

Tutorial 4: Machine Learning Tutorial Python - 4: Gradient Descent and Cost Function
----------------------------------------------------------------------------
We won't we directly using the gradient descent in machine learning.
But if we know this we can understand and we can use the sklearn library in effectively

If school, we will have the equation, based on the equation we can find the values

In ML, we will have the input and out. Based on input and output, we find the equation
Eg: In tutorial 2, we have input and output. (squarefleet and price)
Based on values of input and output list, we can able to find the output if some newput was given

IF we plot the square fleet and price, then we can able to draw the line which can be best fit

This line will be the equation

Goal of this tutorial is find the equation

To find this, lets draw a best fit line.
Lets take a gap of each data point and the line
Collect these data and square them
And then devide them by number of data point
These result is called mean square error.
Mean squalre is also called cost function
Mean spare file

actual data point - predicted data point.Square this.

Gradle descent is technique to find the equltion/formula in efficent way and with less iteration.


To find we will take a 3D graph which has m in one side and b in other side. and we can find mse(mean square error) which will be in height section

We wil start with the 0 as the m and b value to find the mse, then we will give other values of m and b, and we notice mse will fall little down slop.
We can repeat the same multiple times ( We need to take small baby step/slop) and finally we can notice the mse will have the same point with very less different which is global minimum.
That is the answer, that we can use as the m and b in prediction.

We can take multiple m as m1, m2, m3 which might fall on multiple lines. but we need to find the correct mean line
We need to reduce the m and b to find the best fit line. 

We need to also look the graph from b and mse and also form m and mse

If I take fixed difference/step/slop there are chances I might miss the minimum
How I can do that.
I can minimise the step/slop/difference in each time smaller and smaller in each time, that can help to reach the minimum.
We need to also find the slop and the direction also in each and compare where to go

If need we can also use "Essence of calculus" from 3Blue1Brown youtube channel to learn maths things related to this - This is good in explaining in detail


Derivatives is all about slope

To find slope between two points use below formula
slope = change in y / change in x

What if we want to slope in particular point
To calculate we can use below formula
slope = small change in y / small change in x

Here x shrink to 0 and y shrink to 0. Thats where we we will most acurate slope
For equalation link X square (X^2), the slope will be 2X    //IMP: Here we have used ^ where we want to add square or cube above

This is the derivatives
Equation of derivatives is d/dx X2 = 2X

So when X is 2, slope will be 4
When X is 5, slope will be 10


Lets see what is partial derivatives:

Below is the formula for finding partial derivatives

f(x,y)  = X^2 + Y^3      //X square + Y cube .

We can keep other value as 0 and we can find x
f(x,y) = X^2 + 0 = 2X

We can keep oter value s 0 and we can find y
f(x,y) = 0 + Y^3 = 3Y^2



Derivatives of x
f(x) = x^3
d/dx x^3 = 3x^2



f(x,y) = x^3 + y^2

@f/@x = 3x^2 + 0 = 3x^2

@f/@y = 0 + 2y = 2y


In our case, we need to find the partial derivatives of b and paritial derivatives of m // Below are the equations

mse = 1/2 nEi=0 (yi - (mxi + b))^2

@/@m = 2/n nEi=0 -xi (yi - (mxi + b))

@/@b = 2/n nEi=0 -(yi - (mxi + b))


Once we have partial derivatives, we will be having slope which is the direction.
Once we have direction, we have to move a step
For step use below formula
m = m - learning rate * @/@m
b = b - learning rate * @/@b


We will be having b1 value which is the starting point of he slope, then we have next point as b2
b2 = b1 - learning rate * @/@b

We need to reduce the cost as much we can. 
When we get the same cost, that is the answer

Initially we can give less iteration and with higher learning rate difference. Then reduce the learning rate with more decimals
Our goal is to reduce the cost as much. It should not increase.
We should start learning rate with first 0.1. 0.09, 0.08 etc. We need to find the minimum and stick to that learning rate
Once we stick to the learning rate, then increase the iteration count from 10 to 100, or 1000 or 10000
Once we reached the cost will remain nearly same



We can also use math.isClose function to compare whether both values(current and previous cost value) are nearly same or not.
We can give it in if condition and we can break the iterations when close values
Eg:        
if math.isclose(current_cost, previous_cost, rel_tol=1e-20):
   break
For more details about the math.isClose refer below
https://www.w3schools.com/python/ref_math_isclose.asp


------------------------

Tutorial 5: Machine Learning Tutorial Python - 5: Save Model Using Joblib And Pickle
--------------------------------------------------------------------------------------
In this tutorial we can see how we can save the model and we can load the model
There are two ways to save and load the model
1. pickle
2. sklearn joblib or joblib

We can save the trained model


1.pickle: 
//Below are the steps to save the model

import pickle
with open('new_file_name', 'wb') as f:                 //wb means write byte
    pickle.dump(givenModel, f)

Eg:

with open('my_model', 'wb') as f:
    pickle.dump(reg, f)



//Below are the steps to load the saved model

import pickle
with open('file_name', 'rb') as f:                 //rb means read byte
    giveobjName = pickle.load(f)

Eg:
import pickle
with open('my_model', 'rb') as f:  
    ref = pickle.load(f)




2.  sklearn joblib or joblib   //joblib is the latest

//Below are the steps to save the model

from sklearn.external import joblib

joblib.dump(giveModelHere, 'giveFileNameHere')                     # This is to save the model
Eg:
joblib.dump(reg, 'myModel')


//Below are the steps to load the model

from sklearn.external import joblib  //Old
from joblib       //newone

modelObj = joblib.load(giveFileNameHere)                     # This is to load the model
Eg:
model = joblib.load('myModel')


Model will be saved in binary format.
When our model has large numpy array it is suggested to use joblib.
In other cases we can use pickle.
But mosly both works in same way


---------------------

Tutorial 6: Machine Learning Tutorial Python - 6: Dummy Variables & One Hot Encoding
------------------------------------------------------------------------------------
In this tutorial we will cover categorial, dummat variables and one hot encoding
Lets say we have few string values in the table 
Eg: the price of the house price is based on square feet as well as the city. Here city will be in string.
Machines are good in numberic values
We can use each city as each number. Eg: 1, 2, 3.
Machine will also thing this as increasing order or 1+2 =3. This might confuse the machines.

There are two types of categorial variables
1. Nominal - These are not numberic values. Eg: male, felmale, Eg: different colors like white, green, Eg: different cities like delhi, chennai etc
2. Ordinal - These are related to numberic values. Eg: Customer rating 1, 2, 3 Eg: high, medium, low. For these types we can still use number.

Method 1:
For in this case, it is nominal. So we cannot use number value.
To solve this use technique called one hot encoding where we can have different column for each values and have filled with 1 and 0.
Eg:
We can take delhi as one column and we can have 1 value for row for which matches with delhi. For other we can give 0
We can take chennai as one more column. We can give 1 value for row which matches with chennai. For other we can give 0



We have to create multiple columns for each rows with value True/false or 1/0.
For creating multiple columns for each string rows use below
dummies = pd.get_dummies(df.columnName)  # This will just create multiple rows for each string columns
dummies = pd.get_dummies(df.town) # For each town, it will create coumns

Then join both actual and dummies, use below
mergedtable = pd.concat([df, dummies], axis='columns')   # We have to join the columns


Then we have to drop the actual column which has multiple string value.
Then we have to drop one of the newly created column. because that can be derived from the other columns if we make it as false or 0

Below is to drop
finalTable = mergedtable.drop(['town', 'west windsor'], axis='columns') # Here we are dropping town as we have already created column for each value. Also we are dropping west windsor because with previous two column(with value false) it can be derived

Then we can have indepedent variables in one table x
x = finalTable.drop(['price'], axis='columns')

And dependent variable in other table y
y = finalTable.price

We can pass both into the fit method the linear regression for training
Eg:
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x,y)

If we want to get the accuracy rate, we can use score method.
model.score(x,y)   . Here 0.95 means 95% and 1 means 100%

Then for prediction we can use the same along with newly created column value
model.predict([['columnValue1', 'columnValue2', 'columnValue3']])


model.predict([[5000, True, False]])   # This is to find in monroe township
model.predict([[5000, False, True]])  # This is to find in robinsville
model.predict([[5000, False, False]]) # This is to find in west windsor   //We have not created column for west windsor. But by giving false false, we can get the value


Refer jupyter notebook for details

Method 2:
Below is with one hot encoder


df2 = df    # for second method we just using df2
df2

We can use label encoder to give number for each of the string value
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df2.town = le.fit_transform(df['town'])    # This will give number for each value pf town

Then we can assign independent variables to X in array format
X = df2[['town', 'area']].values          #pass the values in array

Then assign dependent price variables to y in array format
y = df2.price.values

Now use one hot encoder to create dummy variables for each of the town    
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer           
ct = ColumnTransformer([('town', OneHotEncoder(), [0])], remainder='passthrough') 

 # IMPORTANT : WE CAN SEE FEW DIFFERENCE IN VIDEO AND THIS CODE, BECAUSE THE VIDEO ONE WAS DEPRECATED       NOW              

Assign X with each created town values of arra to the area. We can find in 2D array
X = ct.fit_transform(X) 

Now drop one of the column. Because one column can be derived from other values
X = X[:,1:]              # Dropping first column (monroe township)

Now train and predict

from sklearn.linear_model import LinearRegression
model2 = LinearRegression()

model2.fit(X,y)    # Training

model2.predict([[0,0,5000]])  # This is to find in monroe township
model2.predict([[1,0,5000]])  # This is to find in robinsville
model2.predict([[0,1,5000]]) # This is to find in west windsor

model2.score(X,y)  # this is to see the accuracy score


We can use any of the two methods

---------------------

Machine Learning Tutorial Python - 7: Training and Testing Data
----------------------------------------------------------------
When we have data for training, it is better to use 80% of the data for training
And remaining 20% of the data we can use for testing for testing

We can use train test split to split the data

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)     //Here it will split 20% for training data and other 60% is for taining, This will return four params like above

Now can use LinearRegression for training

from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(X_train, y_train)
reg.predict(X_test)    //This will show the value. Wont be exactly same. But will be near to the expected value
Now we can compare the predicted value with y_test to see how near it was
We can use score method to check the accuracy score
Using score method we can find the accuracy

----------------------

Machine Learning Tutorial Python - 8: Logistic Regression (Binary Classification)
---------------------------------------------------------------------------------
All above are linear regression where we will find the number/answers based on training. 
But in binary classification (logistic regression) we have few set of answers like yes or no, true or false, (5 or 10 or 15). The prediction is to choose one of the answers.
Logistic regression is the technique used to solve classification problems

There are 2 types of classification
1. Binary classification Eg. This will have only two choice of answer Yes or No
2. Multiclass classification Eg: This will have multiple choice more than two. Eg: Who ther peson will vote. Eg:  dmk, admk, tvk. 


Lets see an example of insurance data based on age.
Lets say we have age as the input and based on age they are buying the insurance.
We can notice when age was higher, they are more probability of buying the insurace.
Lesser the age has less chance to buy the insurace.


For such example we need to plot that in matplotlib scatter chart.
First we can see how the how the linear regression line looks like, we can draw it and we can separate at the middle.
But here there is problem is when some of points are far and if we draw the line and separate.
Few points may not be covered and there are chances we might miss.
So the better way is to draw the curvy line as like screenshot attached in the tutorial for best fit.

In math, we can use sigmoid or logit function is used to find the middle line

Below is the equation of sigmoid function
Sigmoid(z) = 1/1+e^-z
Here the output will be always less than 1
It will be between 0 to 1

We need to use sigmoid function on top of linear regression function. Like below
Sigmoid(z) = 1/1+e^-(mx+b)

These mathamatics is for our understanding.
So that we can understand how internally it works



We need to use below for logistic regression

from sklearn.linear_model import LogisticsRegression
model = LogisticsRegression()

Only above will be different, all other remains same
model.fit(X, y)
model.predict([[giveValueHere]])    
model.score(X, y)


predict_proba function will show prability values for each of the choice
Whether it was near to 0 or 1.
To which it was near to
model.predict_proba([[giveValueHere]])
Eg:
model.predict_proba([[50]])

model.predict_proba(X_test)


-------------------

Tutorail 8 (Part 2) Machine Learning Tutorial Python - 8 Logistic Regression (Multiclass Classification)
---------------------------------------------------------------------
Multiclass classification is the way to predict and choose out of multiple choices
It may not have yes or not.
It will have many choices like who to vote out of 4 parties etc

In this tutorial, we can see pratical example to predict the hand written image(image data) to which number it is. It is 0 or 1  or 2 or 3 or 4 or 5 or 6 or 7 or 8 or 9
Out of 10 numbers it should predict which number it is.

Here we will be using inbuild dataset which has many image data samples

from sklearn.datasets import load_digits          //Here we have data set of handwritten digits

Below has the documentation of load_digits
https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html

This has multiple image samples and its values
from sklearn.datasets import load_digits   # This has handwritten digits
digits = load_digits()

digits.images  //This will have all images and its data in 2 D array and plotted the higher number where it matches
//Even if we print one of the image digits.images[0] and if we shade the lines a per the higher value, we will get near to that number image
To view this, we can use
plt.gray()
plt.matshow(digits.images(0))  //This will plot the values as per array values


But we will use sigle dimension array where all images data are availabel in data
digits.data
All 8 X 8 array will be added in the single dimension array. So in single dimension array there would be 64 values for each

For first image
digits.data[0]

All values will be available in target
digits.target

To view the value, we can use
digits.target[0]  
//This will print first image value

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2)
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=1000) 
model.fit(X_train, y_train)
model.score(X_test, y_test)

We can verify using below
model.predict(digits.data[0:5])
digits.target[0:5]
Check both above outputs are same or not


To view the accurray in the prediction, we can use confusion_matrix, we need to pass the actual and predicted values

y_predicted = model.predict(X_test)
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_predicted)   #Give actual and predicted values
We can view the cm object and we can find the accurray challenges how many times truth and the predicted value

To view in good gui, we  can use below seaborn heatmap and can pass the cm confusion_matrix object
import seaborn as sn
plt.figure(figsize= (10,7))
sn.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

This will show how much is the truth and predicted.
By this we can identify when it was predicted correct and performing accurately and when was bad and not accurate

we sum the correct truth and predicted values
We can sum the missed/incorrect values. And we can find the score here also

We can predict the score using below
Score = Number of correct truth and predicted /Total values
or
Score = (Total values - Number of incorrect values ) / Total values

----------------

Machine Learning Tutorial Python - 9 Decision Tree
----------------------------------------------------
Some time we cannot draw the line base on linear or logical regression.
In that case, we need to go with decision tree algorithim
In this logic it will make the decision based on each value.
And make the decision like a tree

Here first on what basics the order should start.
Based on the order, the performance will be get impacted
First it will check on one of the column and check the entropy, whether the values are same for one of the column or not.
If same, it is low entropy (Low different and mostly same) and this is high information gain and try to start with this as the root tree
Here it will start with the company where for facebook all target values are same

When dealing with decision tree, we will hear about gini impurity which means it is the impurity in the data set

We can use 

from sklearn.preprocessing import LabelEncoder
le_company = LabelEncoder()

df['company_n'] = df.fit_transform(df['company'])
Like wise we need assign numeric values for each values in different columns
Then drop the string columns as we will be using the numeric columns

We can make ready the input and target file

For decision tree, we can use below
from sklearn import tree
model = tree.DecisionTreeClassifier()
model.fit(X, y)    //Pass the input X and target y here
model.score(X, y)
model.predict([[5, 2, 2]])

View jupyter notebook for more details

----------------------


Machine Learning Tutorial Python - 10 Support Vector Machine (SVM)
------------------------------------------------------------------
Support vector machine is one of the technique to solve classification problem in the better way.
It will consider near by splitting plots/points
We need to dray the maximum line helps to split. that will be good to split the line and that works effectively
In 2D we need to draw a line. In 3D we need to split with the plane and for n number of D we cannot imagine. but theoritically and mathematically it was possible to split

We need to understand below 2 
1. Gamma 
2. Regularization

Gamma:
High Gamma: Here we can draw the splitting line near the splitting plots. It may not be straight. Here we will consider only near by splitting plots
Low Gamma: Here will consider near by and far points both. Here we will draw a straight line which splits. 

Both the approaches are fine. But low gamma would be better in many cases. Rarely we will get the failure. But commutaion and overall it would be fine
We might get some error in Low gamma. But stil it was fine and better


Regularization:
High Regularization: Here we will carefully draw the line which splits both
Low Regularization: Here will split with straight line which will fit for splitting

Low regularization might have some error. But still it was better



We might have complex data frame with x y and z
We can apply formula
z = x^2 + y ^2 
(x square + y square)

We can have X and Z in regular and Y will be straight line again.
Once plotted we should be able to get the line


See jupyter notebook for more details

For using SVM use below

from sklearn.svm import svc
model = SVC()
model.fit(X, y)

We can pass the parameter in SVC(), By that we can fine tune the model for more accuracy.

See jyputer notebook for more details


------------------


Machine Learning Tutorial Python - 11 Random Forest
---------------------------------------------------
Here the dataset will be splitted into multiple random slots.
Decision tree will be created for each slots to take the decision.
Based on majority of the decision, prediction will be happen here.
Prediction will be applied based on majority of the decision

Here we will use below to use this:

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(X, y)

We can fine tune with multiple paramters.
Main parameter to fine tune is n_estimators count. Here we can give how many decision tree we need to split up. 
By default, it will be 10. We can increase and fine tune. and we can see which one gives high score


y_prediction = model.predict(X_test)

Once done, we can use confusion metrics to see the accuracy

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_predicted)

We can visualized the confusion matrix on the seaborn for better visualization.

For more details please go throught the jupyter notebook for this tutorial


-----------------


Machine Learning Tutorial Python 12 - K Fold Cross Validation
------------------------------------------------------------

Cross validation is the technique used to evaluate which type of model works better.

We can use any of the 3 validation options, But the 3rd k fold cross validation technique is the better. Below is the reason
Option 1. In first option, we can give all type of data as the input without splitting, we can use the any of the same data to validate. Here the model already knows the answer. So it can predict.
Option 2. In second option, we can split the data into train and test as like above tutorials and we can use train for training and test for testing. It can works mostly. But the problem here is, lets say that test data has some of the complex things which was not covered in the training, that time the model cannot work better
Option 3: This is k fold cross validation. Here, the data will be splitted into slots/folds. Then each one fold with the taken as the each test and rest of the fold will be taken in each train. It will give score for each. And we can get the score and we can average. So this will cover all data as the train and test train and the score will be average. This is better than previous 2. 
Giving few more example. Lets say there are data. Data will be splitted into 5 sets. 1 set will be used as the test and then other 4 will be used the train and it will give the score. Then again, 2nd set will be used as the test and 1st & 3 to 4 sets will be used as the train and get the score. It will be repeated till the let will be used as the test and all above will be used as the train and get the score. It will take the average of all scores and give.
This is K fold cross validation. It is better than the previous for validating which model works better.


We can use KFold or StratifiedKFold like below. But StratifiedKFold is bit better. Because it uses one by one. For KFold uses in random

from sklearn.model_selection import KFold
kf = KFold(n_splits=10)

score_lr = []
score_svc = []
score_rf = []

from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=10) 

for train_index, test_index in skf.split(digits.data, digits.target):
    X_train, X_test, y_train, y_test = digits.data[train_index], digits.data[test_index], digits.target[train_index], digits.target[test_index]
    score_lr.append(get_score(LogisticRegression(max_iter=1000), X_train, X_test, y_train, y_test))
    score_svc.append(get_score(SVC(), X_train, X_test, y_train, y_test))
    score_rf.append(get_score(RandomForestClassifier(n_estimators=200), X_train, X_test, y_train, y_test))

Print below and see the scores
score_lr
score_svc
score_rf

Above is just for explaining about the internal logic.
We can use below cross_val_score which takes care of above


from sklearn.model_selection import cross_val_score
cross_val_score(LogisticRegression(max_iter=1000), digits.data, digits.target)
cross_val_score(SVC(), digits.data, digits.target)
cross_val_score(RandomForestClassifier(n_estimators=50), digits.data, digits.target)

cross_val_score(RandomForestClassifier(n_estimators=100), digits.data, digits.target)
cross_val_score(RandomForestClassifier(n_estimators=150), digits.data, digits.target)
cross_val_score(RandomForestClassifier(n_estimators=200), digits.data, digits.target)

We can give different model and check
We can also give different parameters of the model and we can check. This is parameter turning

Please check jupyter notebook for more details


--------------------

Machine Learning Tutorial Python - 13: K Means Clustering Algorithm
--------------------------------------------------------------------
Machine learning algorithm is categorised into 3
1. Supervised
2. Unsupervised
3. Reinforcement learning

All above tutorials are supervised. Because all above has targets
When we have inputs and targets for ML, it is supervised learning.
Because we know the targets

When we have only inputs, and there is no targets, we can say it is unsupervised.
We are not sure what is the target
KMean is the popular one used for unsupervised learning

Here we will get the data points without target
Plot each data points in scatter.
And visually see how much it can be grouped (cluster)
We can take that as the K value (number of cluster or groups)

Technically to find, we can give 2 different random points we will call it as centroid, 
And calculate the distance of each points with centroid.
We can group based on the points which are closer to each centroid.
Then recalculate and reposition the centroid position based on the groups.
Agan repeat recalculating the distance of each points with new centroid position and regroup it.
Repeat the same again and again until the group not changes.
That is the correct final centroid position.
Here we are supplying K (number of cluster)


But how can we find the K (number of clusters)
We can use one of the technique name elbo method to find the K (number of cluster)
Might be start with 2 as K (number of clusters(groups) as 2) and increase the K and find the SSE (Sum of square error)
Get distance of each data point of the group(for the centroid) with first centroid and sqaure it and then sum
Do the same for other centroid and get SSE for 2.
Do the same by increasing the K one by one until error(SSE) before 0
Now we have SSE for each number of clusters. 
Now look at all SSEs
We can see when increasing the number of cluster, error(difference) decreases
We can also see in one point of K the difference(errors) start reducing. 
That point we can consider as K
If we draw the same, it looks like the hand elbo.
That elbo point is consider as the K
That will be the good K number

Please check the image for the formula


from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler


scaler = MinMaxScaler()           # this will be used to take in the range of 0 to 1. 0 as min and max as 1 and other values will be between 0 and 1. So the pointing will be easy and more clear
scaler.fit(df[['Income($)']])
df['Income($)'] = scaler.transform(df[['Income($)']])

# Doing the same for age
scaler.fit(df[['Age']])
df['Age'] = scaler.transform(df[['Age']])

km = KMeans(n_clusters=3)   # Here we have given number of cluster as 3. But we need to find the number of clusters. I will add that in below after some lines
y_predicted = km.fit_predict(df[['Age', 'Income($)']])
df['predicted'] = y_predicted

km.cluster_centers_   # This will give center(centroid) point of each cluster


To find the K (number of clusters), we can use below elbo method.

k_rng = range(1, 10)
sse = []                  //WE need to take sum of square errors for each k
for k in k_rng:
    km = KMeans(n_clusters=k)
    km.fit(df[['Age', 'Income($)']])
    sse.append(km.inertia_)   #km.inertia will give the sum of square error

Below will print the elbo and we can find the elbo K point.
plt.xlabel('k')
plt.ylabel('sum of square error')
plt.plot(k_rng, sse)


Please check the jupyter notebook for details


------------------

Machine Learning Tutorial Python - 14: Naive Bayes Classifier Algorithm Part 1
-----------------------------------------------------------------------------
We can see some probalility related things

If we swip a coin, the probability of getting head is 1/2 which is 50%
P(head) = 1/2
P(head) = number of head side in coin / total number of sides in coin
P(head) = 1/2



Probability of getting queen in cards
There are four queens are there. 
There are total 52 cards
Below is formula for probability getting queen
P(queen) = number off queens / total number of cards
P(queen) = 4/52
P(queen) = 1/13

Lets say, now we know the card ins diamond, now calculate what is probabilty of getting queen
Total diamond card = 13
Number of queen in diamond = 1
P(queen/diamond)= 1/13
This is conditional probability.
P(A/B) = Probability of event A knowing that event B has already occurred.

Below are the formula:
P(A/B) = (P(B/A) * P(A))/ P(B)


P(queen/diamond) = (P(diamonds/queen) * P(queen))/ P(diamonds)

P(diamonds/queen) = 1/4
P(queen) = 1/13
P(diamond) = 1/4
With the help of other known probabilities, we can find the required probabilty

P(queen/diamond) = ((1/4 * 1/13)) / 1/4
P(queen/diamond) = 1/13



Naive Bayes will use probibility algorithim.
Eg
P(Survived/Male & class & Age & Cabin & Fare)
To find the survived rate, it will use independent featues of male, class, age, cabin and fare

Naive based is used in many places such as email sparm detection, character detection, Weather prediction, face detection, News classification etc


We can create naive bayes model like below
Here we have used Gaussian naive bayes as data distributtion is normal. 
We have used this because the data distribution is normal

from sklearn.naive_bayes import GaussianNB
model = GaussianNB();
model.fit(X_train, y_train)
model.predict(X_test)
model.score(X_test, y_test)

model.predict_proba(X_test) Here it will show the prability value for each

-----------------------


Machine Learning Tutorial Python - 15: Naive Bayes Classifier Algorithm Part 2
------------------------------------------------------------------------------
In this tutorial we can see how we can find whether the email is spam or not.
Here we can also see how we can use the sentences/words in the machine learning.
Here the dataset was string first. It has emails in sentences and whether it is spam or not.

We need to use the technique called count vectorizer technique for text to count. Becuase machine leanring understand only numbers
This will take the unique words on the sentence in the each data and make the count
For each unique words it will create the count

We can use CountVectorizer for getting number of counts of each words in the sentences
from sklearn.feature_extraction.text import CountVectorizer
v = CountVectorizer()
count = v.fit_transform(X.values)


Naive bayes has 3 types of classifier
Bernoulli Naive Bayes : This is used when feature has only 2 or binary in nature. 0 or 1.  yes or not. one or two, true or false. During that we can use this
Multinomial Naive Bayes : It is used when there is discreate data. Example movie rating from 1 to 5 etc
Gaussian Naive Bayes: In last tutorial we have used Gausian Naive Bayes. Mostly used for probabilty related.

Here we will use multinomial naive bayes

Below is the way to use multinomial naive bayes

from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(X_train, y_train)


In above, we want to convert in to count first and then we need to pass it to the model.
To make it simple we can use Pipeline to use multiple steps here.
In below, X_train and y_train will be passed on pipeline which internally calls CountVectorizer first and then pass it will call Multinomial

from sklearn.pipeline import Pipeline
clf = Pipeline([('vectorizer', CountVectorizer()), ('nb', MultinomialNB)])
clf.fit(X_train, y_train)
clf.score(X_test, y_test)

For more detaisl please check the jupyter notebook


-------------------

Machine Learning Tutorial Python - 16: Hyper parameter Tuning (GridSearchCV)
-----------------------------------------------------------------------------
In this tutorial we can see the way to find the best model to be used and with best params to be passed for their params
lets say we want to use iris data set and find the folder type.
First question comes like which model I need to choose. There are so many models like SVM, RandomForst, Logistic Regression, Decision Tree, Naive beyes etc
Lets say we picked one Eg: SVM, then the next question comes like which hyper params I need to use when creating the model.
There are different kernal (rbf, linear, poly), C (Integer) and Gamma (Gamma)  available when creating the model
The process of finding the best params of the model is calling hyper parameter tuning



Below is the way to use GridSearchCV

from sklearn.model_selection import GridSearchCV()
clf = GridSearchCV(giveModelHere, giveParamsHere, cv=giveNumberOfTestIteration, return_train_score=False)
Eg:
from sklearn.model_selection import GridSearchCV
clf = GridSearchCV(SVC(gamma='auto'), {'C':[1, 5, 10, 20], 'kernel':['rbf', 'linear']}, cv=5, return_train_score=False)
clf.fit(iris.data, iris.target)
clf.cv_results_

Below 2 will give you the best score and best params
clf.best_score_
clf.best_params_

Above is hyper parameter turing. We try to find the best params of the model
As GridSearchCV is trying for all permutation and combination, it needs more computation power.

If we worried about computation power, then we can go with RandomizedSearchCV.
This works similar, but for random values for given iteration times.
It won't go with all combination. It will pick few random value and runs only for given number of iteration and gives the results
This helps if we have more params and worrid about computation

from sklearn.model_selection import RandomizedSearchCV
clf = RandomizedSearchCV(SVC(gamma='auto'), {'C':[1, 5, 10, 20], 'kernel':['rbf', 'linear']}, cv=5, return_train_score=False, n_iter=2)    // in n_iter we will give number of iternation needed.
clf.fit(iris.data, iris.target)
clf.cv_results_


For finding the best model, we need to create the dictionary and have the model and the params
And we need to call GridSearchCV or RandomizedSearchCV in each iteration and we can find the best

Example:
from sklear.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

model_data = {
    'svc': {
        'model': SVC(gamma='auto'),
        'params' : {
            'C': [1,10,20],
            'kernel': ['rbf','linear']
        }  
    },
    'random_forest': {
        'model': RandomForestClassifier(),
        'params' : {
            'n_estimators': [1,5,10]
        }
    },
    'logistic_regression' : {
        'model': LogisticRegression(solver='liblinear',multi_class='auto'),
        'params': {
            'C': [1,5,10]
        }
    }
}

scores = []
for mdl_name, mdl in model_data.items():
    clff = GridSearchCV(mdl['model'], mdl['params'], cv=5, return_train_score=False)
    clff.fit(iris.data, iris.target)
    scores.append({
        'model': mdl_name,
        'best_score': clff.best_score_,
        'best_params': clff.best_params_
    })



By this we can see how to find best model

in jupyter notebook we can see more details

-------------------


Machine Learning Tutorial Python - 17: L1 and L2 Regularization | Lasso, Ridge Regression
-----------------------------------------------------------------------------------------
Overfitting is one of the issue in the machine learning.
To overcome or to solve this we use L1 and L2 reguralization

Underfit -  if we train with less data set or if we draw the straight line which divides, it may not sometime predict the right value when the value is in cornor or other ear
Overfit - Whe we train the model with more training set or if we draw the line in each points, it it may not be good. This is overfit
Balanced fit - If we train with certain amount of train set or if we draw the line with balanced and split two, that might be the balanced fit

See image for more details and clarity


Underfit
o + o1 * age

Overfit
o + o1 * age + o2 * age^2 + o3 * age^3 + o4 * age^4

Balanced
o + o1 * age + o2 * age^2

To make the overfitting to balaced fit, you need to make the o3 and o3 closed to 0.
Which will give better results for balanced

There are some formula for L1 and L2 reguralization, Please check the image to see the formula used

Here we can compare normal LinearRegression method with L1 and L2 Regularization method. And we can notice, in most of the case L1 and L2 Regularization works better when compare with normal LinearRegression

We have lesso in sklearn which uses L1 regularization
Below is the link of documentation
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html

from sklearn.linear_model import Lasso
model = Lasso(alpha=50, max_iter=100, tol=0.1)
model.fit(X_train, y_train)
model.score(X_test, y_test)


We have ridge in sklearn which uses L2 regularization
Below is the link of documentation
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html

from sklearn.linear_model import Ridge
model = Ridge(alpha=50, max_iter=100, tol=0.1)
model.fit(X_train, y_train)
model.score(X_test, y_test)


For more details please refer the notebook of this tutorial

--------------------------

Machine Learning Tutorial Python - 18: K nearest neighbors classification with python code
------------------------------------------------------------------------------------------
This is a classification technique which predict based number of nearest values
Here K is the number of nearest value.

Lets say we are taking iris data set and based on length and width of the flower the type might vary
If we plot all in the chat, we can see how it looks like.
Then we should be able to predict if some new point was given

Please refer images for more details

Here we will be giving the K which is number of nearest value.
We will find the nearest values and based on that we will see which type of flower is near to that. 
Based on k number, the highest number is the actual value.
Here we should also consider giving the correct value of K. There are changes only very values data points are available for that type itself.
So in that case, if the K is higher it might predict the wrong value.
So the K value should not be very less and should not be very high



----------------------------

Notes:
-----

Model: Model is the application of machine learning.
Model is the tool which we have trained and ready for use.


Linear regression:
This has few set of input and outputs.
Based on that we need to predict the values for new input


Classification (Logical regression):
Here it has few input and few known outputs.
Here we will be having a choice. It will have the choice like yes or not. Few definied choice.
Based on the input params the answer will be selected amoung the choice









--------------------------

