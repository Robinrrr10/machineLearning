Machine Learning Tutorial Python - 8: Logistic Regression (Binary Classification)
---------------------------------------------------------------------------------
All above are linear regression where we will find the number/answers based on training. 
But in binary classification (logistic regression) we have few set of answers like yes or no, true or false, (5 or 10 or 15). The prediction is to choose one of the answers.
Logistic regression is the technique used to solve classification problems

There are 2 types of classification
1. Binary classification Eg. This will have only two choice of answer Yes or No
2. Multiclass classification Eg: This will have multiple choice more than two. Eg: Who ther peson will vote. Eg:  dmk, admk, tvk. 


Lets see an example of insurance data based on age.
Lets say we have age as the input and based on age they are buying the insurance.
We can notice when age was higher, they are more probability of buying the insurace.
Lesser the age has less chance to buy the insurace.


For such example we need to plot that in matplotlib scatter chart.
First we can see how the how the linear regression line looks like, we can draw it and we can separate at the middle.
But here there is problem is when some of points are far and if we draw the line and separate.
Few points may not be covered and there are chances we might miss.
So the better way is to draw the curvy line as like screenshot attached in the tutorial for best fit.

In math, we can use sigmoid or logit function is used to find the middle line

Below is the equation of sigmoid function
Sigmoid(z) = 1/1+e^-z
Here the output will be always less than 1
It will be between 0 to 1

We need to use sigmoid function on top of linear regression function. Like below
Sigmoid(z) = 1/1+e^-(mx+b)

These mathamatics is for our understanding.
So that we can understand how internally it works


We need to use below for logistic regression

from sklearn.linear_model import LogisticsRegression
model = LogisticsRegression()

Only above will be different, all other remains same
model.fit(X, y)
model.predict([[giveValueHere]])    
model.score(X, y)

predict_proba function will show prability values for each of the choice
Whether it was near to 0 or 1.
To which it was near to
model.predict_proba([[giveValueHere]])
Eg:
model.predict_proba([[50]])

model.predict_proba(X_test)
